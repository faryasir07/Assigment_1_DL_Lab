{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8240,"sourceType":"datasetVersion","datasetId":5504},{"sourceId":12944,"sourceType":"datasetVersion","datasetId":9235},{"sourceId":2664123,"sourceType":"datasetVersion","datasetId":1611656},{"sourceId":2307650,"sourceType":"datasetVersion","datasetId":1391881}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing lib ","metadata":{}},{"cell_type":"code","source":"import re\nimport ast\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, hamming_loss\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\nfrom gensim.models import KeyedVectors\nfrom transformers import BertTokenizer, BertModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T11:59:46.401010Z","iopub.execute_input":"2025-11-28T11:59:46.401286Z","iopub.status.idle":"2025-11-28T11:59:46.406358Z","shell.execute_reply.started":"2025-11-28T11:59:46.401261Z","shell.execute_reply":"2025-11-28T11:59:46.405780Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Config and model","metadata":{}},{"cell_type":"code","source":"# ======================\n# CONFIG\n# ======================\n\n# TODO: change to the actual file path in your Kaggle dataset\nCSV_PATH = r\"/kaggle/input/arxiv-paper-abstracts/arxiv_data_210930-054931.csv\"\n\nEMBEDDING_INFO = {\n    \"word2vec\": {\n        \"path\": r\"/kaggle/input/googlenewsvectors/GoogleNews-vectors-negative300.bin\",\n        \"binary\": True\n    },\n    \"glove\": {\n        \"path\": r\"/kaggle/input/glove6b300dtxt/glove.6B.300d.txt\",\n        \"binary\": False,\n        \"is_glove\": True,\n    },\n    \"fasttext\": {\n        \"path\": r\"/kaggle/input/fasttext-wikinews/wiki-news-300d-1M.vec\",\n        \"binary\": False\n    }\n}\n\nBERT_MODEL_NAME = \"bert-base-uncased\"\nBATCH_SIZE_BERT = 16      # abstracts are long, keep this moderate\nMAX_LEN_BERT = 256\nBATCH_SIZE_TRAIN = 64\nEPOCHS = 8\nLR = 1e-3\nSEED = 42","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T11:59:51.050880Z","iopub.execute_input":"2025-11-28T11:59:51.051474Z","iopub.status.idle":"2025-11-28T11:59:51.055969Z","shell.execute_reply.started":"2025-11-28T11:59:51.051452Z","shell.execute_reply":"2025-11-28T11:59:51.055040Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# device","metadata":{}},{"cell_type":"code","source":"# ======================\n# DEVICE\n# ======================\n\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")\nprint(\"Using device:\", device)\n\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T11:59:53.631563Z","iopub.execute_input":"2025-11-28T11:59:53.632288Z","iopub.status.idle":"2025-11-28T11:59:53.645239Z","shell.execute_reply.started":"2025-11-28T11:59:53.632261Z","shell.execute_reply":"2025-11-28T11:59:53.644605Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# load dataset","metadata":{}},{"cell_type":"code","source":"# ======================\n# LOAD DATA\n# ======================\n\ndf = pd.read_csv(CSV_PATH)\nprint(\"Columns:\", df.columns.tolist())\nprint(\"Shape:\", df.shape)\n\n# Adjust these names if your CSV is slightly different\nTITLE_COL = \"titles\"\nABSTRACT_COL = \"abstracts\"\nTERMS_COL = \"terms\"   # e.g. \"['cs.LG', 'cs.CR', 'stat.ML']\"\n\n# Combine title + abstract\ndf[\"text_full\"] = df[TITLE_COL].fillna(\"\").astype(str) + \" \" + df[ABSTRACT_COL].fillna(\"\").astype(str)\n\n# Drop rows with missing text or labels\ndf = df.dropna(subset=[\"text_full\", TERMS_COL]).copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T11:59:56.529116Z","iopub.execute_input":"2025-11-28T11:59:56.529677Z","iopub.status.idle":"2025-11-28T11:59:58.333307Z","shell.execute_reply.started":"2025-11-28T11:59:56.529639Z","shell.execute_reply":"2025-11-28T11:59:58.332508Z"}},"outputs":[{"name":"stdout","text":"Columns: ['terms', 'titles', 'abstracts']\nShape: (56181, 3)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# label preprocessing","metadata":{}},{"cell_type":"code","source":"# =====================================\n# LABEL PROCESSING (MULTI-LABEL)\n# =====================================\n\n# Convert string -> Python list using ast.literal_eval\ndf[\"parsed_terms\"] = df[TERMS_COL].apply(lambda x: ast.literal_eval(str(x)))\n\n# MultiLabelBinarizer: list of labels -> multi-hot vector\nmlb = MultiLabelBinarizer()\nlabels = mlb.fit_transform(df[\"parsed_terms\"]).astype(\"float32\")\n\nLABELS = list(mlb.classes_)\nnum_labels = len(LABELS)\n\ntexts = df[\"text_full\"].astype(str).tolist()\n\nprint(\"Number of unique labels:\", num_labels)\nprint(\"First 20 labels:\", LABELS[:20])\nprint(\"Labels shape:\", labels.shape)\nprint(\"Example text:\", texts[0][:200], \"...\")\nprint(\"Example label vector sum (how many labels):\", labels[0].sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T12:00:01.618883Z","iopub.execute_input":"2025-11-28T12:00:01.619531Z","iopub.status.idle":"2025-11-28T12:00:02.428855Z","shell.execute_reply.started":"2025-11-28T12:00:01.619504Z","shell.execute_reply":"2025-11-28T12:00:02.428181Z"}},"outputs":[{"name":"stdout","text":"Number of unique labels: 1177\nFirst 20 labels: ['00', '00-02', '00B25', '00Bxx', '03B52, 94A08', '03B70, 03-04, 03D10, 11Y16', '05B45, 62H30, 54E05, 68T10', '05C20, 14T10, 62G32, 62H22, 05C99, 62R01, 65S05', '05C21, 68T07, 76D07, 76M10, 76S05', '05C50', '05C50, 05C70, 65M55', '05C50, 68T07', '05C60', '05C62, 41A46, 41A63, 62J02', '05C85, 05C80', '05C99, 62M45', '06A15, 06B99, 68T05, 91A80', '10010147.10010257.10010258.10010259.10010263', '11Z05', '14J60']\nLabels shape: (56181, 1177)\nExample text: Multi-Level Attention Pooling for Graph Neural Networks: Unifying Graph Representations with Multiple Localities Graph neural networks (GNNs) have been widely used to learn vector\nrepresentation of gr ...\nExample label vector sum (how many labels): 1.0\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# train and test split ","metadata":{}},{"cell_type":"code","source":"# ======================\n# TRAIN / VAL / TEST SPLIT\n# ======================\n\nindices = np.arange(len(texts))\nidx_train, idx_temp, y_train, y_temp = train_test_split(\n    indices, labels, test_size=0.25, random_state=SEED\n)\nidx_val, idx_test, y_val, y_test = train_test_split(\n    idx_temp, y_temp, test_size=0.5, random_state=SEED\n)\n\ntexts = np.array(texts)\nX_train_text = texts[idx_train]\nX_val_text = texts[idx_val]\nX_test_text = texts[idx_test]\n\nprint(f\"Train: {len(X_train_text)}, Val: {len(X_val_text)}, Test: {len(X_test_text)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T12:00:06.779228Z","iopub.execute_input":"2025-11-28T12:00:06.779854Z","iopub.status.idle":"2025-11-28T12:00:07.957000Z","shell.execute_reply.started":"2025-11-28T12:00:06.779830Z","shell.execute_reply":"2025-11-28T12:00:07.956193Z"}},"outputs":[{"name":"stdout","text":"Train: 42135, Val: 7023, Test: 7023\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# text cleaning and tokenization","metadata":{}},{"cell_type":"code","source":"# ======================\n# TEXT CLEANING / TOKENIZATION\n# ======================\n\nimport html\nimport emoji\n\nCONTRACTIONS = {\n    \"can't\": \"cannot\",\n    \"won't\": \"will not\",\n    \"don't\": \"do not\",\n    \"doesn't\": \"does not\",\n    \"didn't\": \"did not\",\n    \"isn't\": \"is not\",\n    \"aren't\": \"are not\",\n    \"weren't\": \"were not\",\n    \"wasn't\": \"was not\",\n    \"i'm\": \"i am\",\n    \"you're\": \"you are\",\n    \"we're\": \"we are\",\n    \"they're\": \"they are\",\n    \"it's\": \"it is\",\n    \"that's\": \"that is\",\n    \"there's\": \"there is\",\n    \"i've\": \"i have\",\n    \"you've\": \"you have\",\n    \"we've\": \"we have\",\n    \"they've\": \"they have\",\n    \"i'll\": \"i will\",\n    \"you'll\": \"you will\",\n    \"we'll\": \"we will\",\n    \"they'll\": \"they will\",\n    \"i'd\": \"i would\",\n    \"you'd\": \"you would\",\n    \"we'd\": \"we would\",\n    \"they'd\": \"they would\",\n}\n\nSTOPWORDS = {\n    \"the\", \"a\", \"an\", \"of\", \"and\", \"to\", \"in\", \"on\", \"for\", \"with\", \"at\", \"by\",\n    \"this\", \"that\", \"these\", \"those\",\n    \"is\", \"am\", \"are\", \"was\", \"were\", \"be\", \"been\",\n    \"it\", \"its\", \"as\", \"or\", \"so\",\n    \"do\", \"does\", \"did\",\n    \"you\", \"i\", \"we\", \"they\", \"he\", \"she\", \"him\", \"her\", \"them\", \"our\", \"your\",\n    \"from\", \"about\", \"into\", \"out\", \"up\", \"down\",\n    \"just\", \"very\", \"too\"\n}\n\ndef expand_contractions(text: str) -> str:\n    for contr, full in CONTRACTIONS.items():\n        text = re.sub(r\"\\b\" + re.escape(contr) + r\"\\b\", full, text)\n    return text\n\ndef clean_text(text: str) -> str:\n    text = str(text)\n    text = html.unescape(text)\n    text = text.lower()\n    text = expand_contractions(text)\n    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n    text = re.sub(r\"@\\w+\", \" \", text)\n    text = re.sub(r\"#(\\w+)\", r\"\\1\", text)\n    text = emoji.replace_emoji(text, replace=\" \")\n    text = re.sub(r\"[^a-z0-9\\s']\", \" \", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\ndef simple_tokenize(text: str):\n    text = clean_text(text)\n    tokens = text.split()\n    tokens = [t for t in tokens if t not in STOPWORDS]\n    return tokens\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T12:00:10.603719Z","iopub.execute_input":"2025-11-28T12:00:10.603992Z","iopub.status.idle":"2025-11-28T12:00:10.653468Z","shell.execute_reply.started":"2025-11-28T12:00:10.603972Z","shell.execute_reply":"2025-11-28T12:00:10.652930Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# static embedding","metadata":{}},{"cell_type":"code","source":"# ======================\n# STATIC EMBEDDINGS\n# ======================\n\ndef load_keyed_vectors(info):\n    path = info[\"path\"]\n    binary = info.get(\"binary\", False)\n    is_glove = info.get(\"is_glove\", False)\n\n    if is_glove:\n        print(f\"Converting GloVe from {path} to word2vec format...\")\n        converted_path = \"/kaggle/working/glove_converted.txt\"\n        from gensim.scripts.glove2word2vec import glove2word2vec\n        glove2word2vec(path, converted_path)\n        path = converted_path\n        binary = False\n\n    print(f\"Loading embeddings from {path} (binary={binary}) ...\")\n    kv = KeyedVectors.load_word2vec_format(path, binary=binary)\n    print(\"Embedding dim:\", kv.vector_size)\n    return kv\ndef compute_doc_embeddings(text_list, kv):\n    dim = kv.vector_size\n    embs = []\n    for text in tqdm(text_list, desc=\"Computing doc embeddings\"):\n        toks = simple_tokenize(text)\n        vecs = [kv[w] for w in toks if w in kv]\n        if len(vecs) == 0:\n            embs.append(np.zeros(dim, dtype=\"float32\"))\n        else:\n            embs.append(np.mean(vecs, axis=0).astype(\"float32\"))\n    return np.vstack(embs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T12:00:12.810370Z","iopub.execute_input":"2025-11-28T12:00:12.810993Z","iopub.status.idle":"2025-11-28T12:00:12.817107Z","shell.execute_reply.started":"2025-11-28T12:00:12.810967Z","shell.execute_reply":"2025-11-28T12:00:12.816370Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# bert embedding","metadata":{}},{"cell_type":"code","source":"# ======================\n# BERT EMBEDDINGS (GPU)\n# ======================\n\ntokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\nbert_model = BertModel.from_pretrained(BERT_MODEL_NAME).to(device)\nbert_model.eval()\nfor p in bert_model.parameters():\n    p.requires_grad = False\n\n@torch.no_grad()\ndef compute_bert_embeddings(text_list):\n    embs = []\n    for i in tqdm(range(0, len(text_list), BATCH_SIZE_BERT), desc=\"BERT embeddings\"):\n        raw_batch = text_list[i : i + BATCH_SIZE_BERT]\n        batch_texts = [clean_text(t) for t in raw_batch]\n        enc = tokenizer(\n            batch_texts,\n            padding=True,\n            truncation=True,\n            max_length=MAX_LEN_BERT,\n            return_tensors=\"pt\"\n        )\n        enc = {k: v.to(device) for k, v in enc.items()}\n        out = bert_model(**enc)\n        cls_emb = out.last_hidden_state[:, 0, :]  # [CLS]\n        embs.append(cls_emb.cpu().numpy())\n    return np.vstack(embs).astype(\"float32\")\n# ======================\n# MODEL & TRAINING (MULTI-LABEL)\n# ======================\n\nclass MLPClassifier(nn.Module):\n    def __init__(self, input_dim, num_labels):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, num_labels)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\ndef train_and_eval(X_train, y_train, X_val, y_val, X_test, y_test, name):\n    X_train_t = torch.tensor(X_train, dtype=torch.float32)\n    y_train_t = torch.tensor(y_train, dtype=torch.float32)\n    X_val_t = torch.tensor(X_val, dtype=torch.float32)\n    y_val_t = torch.tensor(y_val, dtype=torch.float32)\n    X_test_t = torch.tensor(X_test, dtype=torch.float32)\n    y_test_t = torch.tensor(y_test, dtype=torch.float32)\n\n    train_ds = TensorDataset(X_train_t, y_train_t)\n    val_ds = TensorDataset(X_val_t, y_val_t)\n    test_ds = TensorDataset(X_test_t, y_test_t)\n\n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE_TRAIN, shuffle=False)\n    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE_TRAIN, shuffle=False)\n\n    input_dim = X_train.shape[1]\n    num_labels = y_train.shape[1]\n    model = MLPClassifier(input_dim, num_labels).to(device)\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n\n    best_val_micro = 0.0\n    best_state = None\n\n    for epoch in range(1, EPOCHS + 1):\n        model.train()\n        total_loss = 0.0\n        for xb, yb in train_loader:\n            xb = xb.to(device)\n            yb = yb.to(device)\n\n            optimizer.zero_grad()\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * xb.size(0)\n\n        avg_loss = total_loss / len(train_ds)\n    # Validation\n        model.eval()\n        all_y = []\n        all_pred = []\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device)\n                yb = yb.to(device)\n                logits = model(xb)\n                probs = torch.sigmoid(logits)\n                preds = (probs >= 0.5).float()\n                all_y.append(yb.cpu().numpy())\n                all_pred.append(preds.cpu().numpy())\n        y_true = np.vstack(all_y)\n        y_pred = np.vstack(all_pred)\n\n        micro_f1 = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n        macro_f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n        h_loss = hamming_loss(y_true, y_pred)\n\n        if micro_f1 > best_val_micro:\n            best_val_micro = micro_f1\n            best_state = model.state_dict()\n\n        print(f\"[{name}] Epoch {epoch}/{EPOCHS} | \"\n              f\"Train loss {avg_loss:.4f} | \"\n              f\"Val micro-F1 {micro_f1:.4f} | \"\n              f\"Val macro-F1 {macro_f1:.4f} | \"\n              f\"Val Hamming {h_loss:.4f}\")\n    # Load best model and evaluate on test\n    if best_state is not None:\n        model.load_state_dict(best_state)\n\n    model.eval()\n    all_y = []\n    all_pred = []\n    with torch.no_grad():\n        for xb, yb in test_loader:\n            xb = xb.to(device)\n            yb = yb.to(device)\n            logits = model(xb)\n            probs = torch.sigmoid(logits)\n            preds = (probs >= 0.5).float()\n            all_y.append(yb.cpu().numpy())\n            all_pred.append(preds.cpu().numpy())\n    y_true = np.vstack(all_y)\n    y_pred = np.vstack(all_pred)\n\n    micro_f1 = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n    macro_f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n    h_loss = hamming_loss(y_true, y_pred)\n\n    print(f\"\\n==== TEST RESULTS: {name} ====\")\n    print(f\"Micro-F1 : {micro_f1:.4f}\")\n    print(f\"Macro-F1 : {macro_f1:.4f}\")\n    print(f\"Hamming  : {h_loss:.4f}\\n\")\n\n    return micro_f1, macro_f1, h_loss\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T12:00:15.824169Z","iopub.execute_input":"2025-11-28T12:00:15.824896Z","iopub.status.idle":"2025-11-28T12:00:19.190053Z","shell.execute_reply.started":"2025-11-28T12:00:15.824870Z","shell.execute_reply":"2025-11-28T12:00:19.189438Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23b655503cfb4442acc2ce21a348a203"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e58671a66054b7e923e253cb0bfaa9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"379d95057d6e46628664d9509aed9454"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72f25a34182f4292aa2c7aa245b5dec9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf3ee4ba122e4b1a93c52fc035fe7aa9"}},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"# run models","metadata":{}},{"cell_type":"code","source":"# ======================\n# RUN 4 MODELS\n# ======================\n\nresults = {}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T12:00:24.461893Z","iopub.execute_input":"2025-11-28T12:00:24.462181Z","iopub.status.idle":"2025-11-28T12:00:24.465628Z","shell.execute_reply.started":"2025-11-28T12:00:24.462152Z","shell.execute_reply":"2025-11-28T12:00:24.465074Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# model - Word2Vec","metadata":{}},{"cell_type":"code","source":"# 1) Word2Vec\nkv_w2v = load_keyed_vectors(EMBEDDING_INFO[\"word2vec\"])\nX_train_w2v = compute_doc_embeddings(X_train_text, kv_w2v)\nX_val_w2v = compute_doc_embeddings(X_val_text, kv_w2v)\nX_test_w2v = compute_doc_embeddings(X_test_text, kv_w2v)\nresults[\"Word2Vec\"] = train_and_eval(X_train_w2v, y_train, X_val_w2v, y_val, X_test_w2v, y_test, \"Word2Vec\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T12:03:36.195276Z","iopub.execute_input":"2025-11-28T12:03:36.195570Z","iopub.status.idle":"2025-11-28T12:06:26.211622Z","shell.execute_reply.started":"2025-11-28T12:03:36.195546Z","shell.execute_reply":"2025-11-28T12:06:26.210926Z"}},"outputs":[{"name":"stdout","text":"Loading embeddings from /kaggle/input/googlenewsvectors/GoogleNews-vectors-negative300.bin (binary=True) ...\nEmbedding dim: 300\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Computing doc embeddings:   0%|          | 0/42135 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d093ebe068240869e721f7745e66b14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Computing doc embeddings:   0%|          | 0/7023 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e166f8ec1014514aa5a906d32a7af29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Computing doc embeddings:   0%|          | 0/7023 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5d8f7c154e7419baf0c7d28a7475830"}},"metadata":{}},{"name":"stdout","text":"[Word2Vec] Epoch 1/8 | Train loss 0.0271 | Val micro-F1 0.5871 | Val macro-F1 0.0013 | Val Hamming 0.0013\n[Word2Vec] Epoch 2/8 | Train loss 0.0039 | Val micro-F1 0.6686 | Val macro-F1 0.0020 | Val Hamming 0.0010\n[Word2Vec] Epoch 3/8 | Train loss 0.0037 | Val micro-F1 0.6760 | Val macro-F1 0.0020 | Val Hamming 0.0010\n[Word2Vec] Epoch 4/8 | Train loss 0.0036 | Val micro-F1 0.6819 | Val macro-F1 0.0020 | Val Hamming 0.0009\n[Word2Vec] Epoch 5/8 | Train loss 0.0036 | Val micro-F1 0.6798 | Val macro-F1 0.0020 | Val Hamming 0.0009\n[Word2Vec] Epoch 6/8 | Train loss 0.0035 | Val micro-F1 0.6826 | Val macro-F1 0.0020 | Val Hamming 0.0009\n[Word2Vec] Epoch 7/8 | Train loss 0.0034 | Val micro-F1 0.6826 | Val macro-F1 0.0020 | Val Hamming 0.0009\n[Word2Vec] Epoch 8/8 | Train loss 0.0034 | Val micro-F1 0.6840 | Val macro-F1 0.0021 | Val Hamming 0.0009\n\n==== TEST RESULTS: Word2Vec ====\nMicro-F1 : 0.6809\nMacro-F1 : 0.0020\nHamming  : 0.0009\n\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# model - GloVe","metadata":{}},{"cell_type":"code","source":"# 2) GloVe\nkv_glove = load_keyed_vectors(EMBEDDING_INFO[\"glove\"])\nX_train_glove = compute_doc_embeddings(X_train_text, kv_glove)\nX_val_glove = compute_doc_embeddings(X_val_text, kv_glove)\nX_test_glove = compute_doc_embeddings(X_test_text, kv_glove)\nresults[\"GloVe\"] = train_and_eval(X_train_glove, y_train, X_val_glove, y_val, X_test_glove, y_test, \"GloVe\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T12:06:33.486850Z","iopub.execute_input":"2025-11-28T12:06:33.487148Z","iopub.status.idle":"2025-11-28T12:13:46.376872Z","shell.execute_reply.started":"2025-11-28T12:06:33.487125Z","shell.execute_reply":"2025-11-28T12:13:46.375978Z"}},"outputs":[{"name":"stdout","text":"Converting GloVe from /kaggle/input/glove6b300dtxt/glove.6B.300d.txt to word2vec format...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_47/2594457065.py:14: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n  glove2word2vec(path, converted_path)\n","output_type":"stream"},{"name":"stdout","text":"Loading embeddings from /kaggle/working/glove_converted.txt (binary=False) ...\nEmbedding dim: 300\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Computing doc embeddings:   0%|          | 0/42135 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27e7e22c3af647fe9b62a9599afec51a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Computing doc embeddings:   0%|          | 0/7023 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ce5181a48054a3996efc9ef79e2528b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Computing doc embeddings:   0%|          | 0/7023 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36c69e2261844867998e0965829a2faa"}},"metadata":{}},{"name":"stdout","text":"[GloVe] Epoch 1/8 | Train loss 0.0210 | Val micro-F1 0.6646 | Val macro-F1 0.0019 | Val Hamming 0.0010\n[GloVe] Epoch 2/8 | Train loss 0.0036 | Val micro-F1 0.6797 | Val macro-F1 0.0020 | Val Hamming 0.0009\n[GloVe] Epoch 3/8 | Train loss 0.0035 | Val micro-F1 0.6746 | Val macro-F1 0.0020 | Val Hamming 0.0009\n[GloVe] Epoch 4/8 | Train loss 0.0034 | Val micro-F1 0.6839 | Val macro-F1 0.0021 | Val Hamming 0.0009\n[GloVe] Epoch 5/8 | Train loss 0.0033 | Val micro-F1 0.6805 | Val macro-F1 0.0021 | Val Hamming 0.0009\n[GloVe] Epoch 6/8 | Train loss 0.0033 | Val micro-F1 0.6868 | Val macro-F1 0.0021 | Val Hamming 0.0009\n[GloVe] Epoch 7/8 | Train loss 0.0032 | Val micro-F1 0.6881 | Val macro-F1 0.0022 | Val Hamming 0.0009\n[GloVe] Epoch 8/8 | Train loss 0.0032 | Val micro-F1 0.6838 | Val macro-F1 0.0022 | Val Hamming 0.0009\n\n==== TEST RESULTS: GloVe ====\nMicro-F1 : 0.6850\nMacro-F1 : 0.0022\nHamming  : 0.0009\n\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# model - FastText","metadata":{}},{"cell_type":"code","source":"# 3) FastText\nkv_ft = load_keyed_vectors(EMBEDDING_INFO[\"fasttext\"])\nX_train_ft = compute_doc_embeddings(X_train_text, kv_ft)\nX_val_ft = compute_doc_embeddings(X_val_text, kv_ft)\nX_test_ft = compute_doc_embeddings(X_test_text, kv_ft)\nresults[\"FastText\"] = train_and_eval(X_train_ft, y_train, X_val_ft, y_val, X_test_ft, y_test, \"FastText\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T12:13:46.378347Z","iopub.execute_input":"2025-11-28T12:13:46.378610Z","iopub.status.idle":"2025-11-28T12:20:57.714282Z","shell.execute_reply.started":"2025-11-28T12:13:46.378590Z","shell.execute_reply":"2025-11-28T12:20:57.713545Z"}},"outputs":[{"name":"stdout","text":"Loading embeddings from /kaggle/input/fasttext-wikinews/wiki-news-300d-1M.vec (binary=False) ...\nEmbedding dim: 300\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Computing doc embeddings:   0%|          | 0/42135 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02913e61adae468d8e82599c2b234a46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Computing doc embeddings:   0%|          | 0/7023 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebd027b8cc4740b8b44a1401e0386098"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Computing doc embeddings:   0%|          | 0/7023 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56f2fac123ab4bb7856956c679b9ca29"}},"metadata":{}},{"name":"stdout","text":"[FastText] Epoch 1/8 | Train loss 0.0300 | Val micro-F1 0.6289 | Val macro-F1 0.0015 | Val Hamming 0.0010\n[FastText] Epoch 2/8 | Train loss 0.0038 | Val micro-F1 0.6730 | Val macro-F1 0.0019 | Val Hamming 0.0009\n[FastText] Epoch 3/8 | Train loss 0.0036 | Val micro-F1 0.6807 | Val macro-F1 0.0020 | Val Hamming 0.0009\n[FastText] Epoch 4/8 | Train loss 0.0036 | Val micro-F1 0.6887 | Val macro-F1 0.0020 | Val Hamming 0.0009\n[FastText] Epoch 5/8 | Train loss 0.0036 | Val micro-F1 0.6866 | Val macro-F1 0.0020 | Val Hamming 0.0009\n[FastText] Epoch 6/8 | Train loss 0.0035 | Val micro-F1 0.6897 | Val macro-F1 0.0020 | Val Hamming 0.0009\n[FastText] Epoch 7/8 | Train loss 0.0035 | Val micro-F1 0.6770 | Val macro-F1 0.0020 | Val Hamming 0.0009\n[FastText] Epoch 8/8 | Train loss 0.0034 | Val micro-F1 0.6837 | Val macro-F1 0.0020 | Val Hamming 0.0009\n\n==== TEST RESULTS: FastText ====\nMicro-F1 : 0.6847\nMacro-F1 : 0.0020\nHamming  : 0.0009\n\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# model - Bert","metadata":{}},{"cell_type":"code","source":"# 4) BERT\nX_train_bert = compute_bert_embeddings(list(X_train_text))\nX_val_bert = compute_bert_embeddings(list(X_val_text))\nX_test_bert = compute_bert_embeddings(list(X_test_text))\nresults[\"BERT\"] = train_and_eval(X_train_bert, y_train, X_val_bert, y_val, X_test_bert, y_test, \"BERT\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T12:20:57.715184Z","iopub.execute_input":"2025-11-28T12:20:57.715494Z","iopub.status.idle":"2025-11-28T12:38:41.454285Z","shell.execute_reply.started":"2025-11-28T12:20:57.715471Z","shell.execute_reply":"2025-11-28T12:38:41.453601Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"BERT embeddings:   0%|          | 0/2634 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1335dbcc140c4e91a12048f151c51099"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"BERT embeddings:   0%|          | 0/439 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac9d206a41a74300b3c2bbe8318992e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"BERT embeddings:   0%|          | 0/439 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fce576683e44474e95c194d79d0eff23"}},"metadata":{}},{"name":"stdout","text":"[BERT] Epoch 1/8 | Train loss 0.0113 | Val micro-F1 0.6552 | Val macro-F1 0.0019 | Val Hamming 0.0010\n[BERT] Epoch 2/8 | Train loss 0.0037 | Val micro-F1 0.6695 | Val macro-F1 0.0019 | Val Hamming 0.0009\n[BERT] Epoch 3/8 | Train loss 0.0035 | Val micro-F1 0.6840 | Val macro-F1 0.0020 | Val Hamming 0.0009\n[BERT] Epoch 4/8 | Train loss 0.0034 | Val micro-F1 0.6734 | Val macro-F1 0.0020 | Val Hamming 0.0009\n[BERT] Epoch 5/8 | Train loss 0.0033 | Val micro-F1 0.6842 | Val macro-F1 0.0021 | Val Hamming 0.0009\n[BERT] Epoch 6/8 | Train loss 0.0032 | Val micro-F1 0.6862 | Val macro-F1 0.0022 | Val Hamming 0.0009\n[BERT] Epoch 7/8 | Train loss 0.0031 | Val micro-F1 0.6929 | Val macro-F1 0.0025 | Val Hamming 0.0009\n[BERT] Epoch 8/8 | Train loss 0.0031 | Val micro-F1 0.6934 | Val macro-F1 0.0028 | Val Hamming 0.0009\n\n==== TEST RESULTS: BERT ====\nMicro-F1 : 0.6939\nMacro-F1 : 0.0028\nHamming  : 0.0009\n\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"print(\"==== SUMMARY (Micro-F1, Macro-F1, Hamming) ====\")\nfor name, (mi, ma, h) in results.items():\n    print(f\"{name:8s} -> Micro-F1 {mi:.4f} | Macro-F1 {ma:.4f} | Hamming {h:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T12:44:07.591704Z","iopub.execute_input":"2025-11-28T12:44:07.592460Z","iopub.status.idle":"2025-11-28T12:44:07.596951Z","shell.execute_reply.started":"2025-11-28T12:44:07.592434Z","shell.execute_reply":"2025-11-28T12:44:07.596155Z"}},"outputs":[{"name":"stdout","text":"==== SUMMARY (Micro-F1, Macro-F1, Hamming) ====\nWord2Vec -> Micro-F1 0.6809 | Macro-F1 0.0020 | Hamming 0.0009\nGloVe    -> Micro-F1 0.6850 | Macro-F1 0.0022 | Hamming 0.0009\nFastText -> Micro-F1 0.6847 | Macro-F1 0.0020 | Hamming 0.0009\nBERT     -> Micro-F1 0.6939 | Macro-F1 0.0028 | Hamming 0.0009\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}